import pymongo, sys, re
from pymongo import MongoClient
import datetime, random
from insertSomeData import take_input

client = MongoClient('mongodb://localhost', 27017)

# create a db
db = client.urls

"""
Creating a collection. Collection is equivalent to table name in SQL
A collection is a group of documents stored in MongoDB
"""

urlCollection = db.top100urls

"""
Create documents in collection.
An important note about collections (and databases) in MongoDB is that 
they are created lazily - none of the above commands have actually 
performed any operations on the MongoDB server. Collections and 
databases are created when the first document is inserted into them.
"""


class shortenSaveUrl:
    imported = False

    def __init__(self):
        self.urlsList = list()
        self.final_list = list()

    @classmethod
    def shorten_url(cls, url):
        jumbled = url[len('https://'):]

        rng = random.sample(range(1, len(jumbled)), len(jumbled) // 4)

        lstchanged = list()

        for idx, each in enumerate(rng):
            if idx % 2 == 0 and not idx % 3 == 0:
                lstchanged.append(random.randint(48, 57))
            elif idx % 3 == 0 and not idx % 2 == 0:
                lstchanged.append(random.randint(65, 90))
            else:
                lstchanged.append(random.randint(97, 122))

        """
        48-57(0-9) 65-90(A-Z) 97-122(a-z) Ascii range for
        numbers and letters.
        """
        chrlst = list(map(lambda x: chr(x), lstchanged))
        newurl = url[:8] + "".join(chrlst)
        yield newurl

    def read_urls_from_file(self, filename):
        with open(filename, 'r') as top:
            self.urlsList = top.readlines()
        return self.urlsList

    def prepare_doc_to_insert(self, urls_list):
        """
        fields/keys in the docs as follows:
        _id:       autogenerated
        name:      name of the url
        org_url:   the original url
        short_url: the shortened url
        when:      time when the url was added
        """
        try:
            for item in urls_list:
                temp_doc = dict()
                """
                l = 'https://docs.python.org/3/library/re.html#module-re'
                m = re.search('[a-zA-Z0-9]/[a-zA-Z0-9]', l)
                m.span() --> returns tuple with start and end pos
                             (22, 25)
                we need the last value in the tuple so m.span()[1]-2
                str = l[:m.span()[1]-1]
                print(str) -->  https://docs.python.org
                """
                search_obj = re.search('[a-zA-Z0-9]/[a-zA-Z0-9]*', item)
                assert search_obj != None
                end_pos = search_obj.span()[1] -1
                part_1 = item[:end_pos]
                print(part_1)
                temp_doc['name'] = part_1
                temp_doc['org_url'] = item.strip("\n")
                temp_doc['org_len'] = len(item)
                shortened = shortenSaveUrl.shorten_url(item)
                temp_doc['short_url'] = next(shortened)
                temp_doc['short_len'] = len(temp_doc['short_url'])
                now = datetime.datetime.now()
                temp_doc['when'] = now.strftime("%m-%d-%Y %H:%M:%S")
                self.final_list.append(temp_doc)
        except AssertionError as error:
            print("Couldn't extract some fields")
        return self.final_list

    @staticmethod
    def insert_many_urls(many_urls):
        urlCollection.insert_many(many_urls)

    @staticmethod
    def insert_one_url(url_to_insert):
        urlCollection.insert_one(url_to_insert)

    @staticmethod
    def find_by_part_name(searchparam):
        found_docs = list(urlCollection.find({'$text': {'$search': searchparam}}))
        print("Found %d docs matching the querry" %len(found_docs))
        for item in found_docs:
            print("URL: ", item['org_url'])

    @staticmethod
    def creating_index():
        urlCollection.create_index([('org_url', pymongo.TEXT)], unique=True)

def add_url():
    workWithUrls = shortenSaveUrl()
    # Paste or type the URL(s) to insert into the DB
    print("Paste/Type the url to enter")
    user_input = str(input())

    # Making a dictionary from user input
    list_urls = workWithUrls.prepare_doc_to_insert([user_input])

    # Insert a single url into the DB
    workWithUrls.insert_one_url(list_urls[0])


def search_by_name():
    print("\tEnter the keyword for the url")
    search_name = str(input()).lower()
    search = shortenSaveUrl()
    search.find_by_part_name(search_name)


def what_next():
    print("What you want to do ?")
    print("\tAdd new url to DB or Search a url in the DB")
    print("\tType \"add\" or \"search\" ")
    print("\tType quit/Quit/q to quit")
    user_input = str(input())

    if user_input.lower() == "add":
        add_url()
    elif user_input.lower() == "search":
        search_by_name()
    elif user_input.lower() == "q" or user_input == "quit":
        sys.exit(1)
    else:
        what_next()

if __name__ == "__main__":
    if not urlCollection.find({}).count() >= 100:
        take_input('top100.txt')
    while True:
        what_next()

# take filename as input to insert bulk
# take one or more url separated by comma
# avoid duplicates SOLU: Creates a unique index to avoid adding duplicates
